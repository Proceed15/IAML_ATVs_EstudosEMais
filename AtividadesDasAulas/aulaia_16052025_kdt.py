# -*- coding: utf-8 -*-
"""AulaIA_16052025_KDT.ipynb

Automatically generated by Colab.

Original file is located at Google Colab
"""



# Instalação necessária (no Colab ou terminal)
!pip install -q spacy transformers sentence-transformers

import spacy
import nltk
import numpy as np
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from nltk.tokenize import sent_tokenize

# Downloads necessários
nltk.download("punkt")
nltk.download('punkt_tab')
nlp = spacy.load("en_core_web_sm")

# Texto longo para A Extração de Informações
text = '''
Artificial Intelligence is transforming the world. Companies like OpenAI, Google, and Microsoft are investing heavily in AI technologies.
Elon Musk, CEO of Tesla and SpaceX, has also expressed both excitement and concern over the future of AI.
Meanwhile, products such as ChatGPT and autonomous cars are becoming more mainstream.
This rapid advancement raises ethical questions about privacy, employment, and decision-making.
Some believe AI will make life easier, while others fear it might replace humans in critical roles.
Despite concerns, the technology continues to evolve, gaining popularity in industries like healthcare, education, and entertainment.
'''

# 1. Extração de Informação com spaCy
print("\n[1] Extração de Informação (NER com spaCy):")
doc = nlp(text)
for ent in doc.ents:
    print(f"- {ent.text} ({ent.label_})")

print("\nRelações simples extraídas:")
for token in doc:
    if token.dep_ == "ROOT":
        subject = [w for w in token.lefts if w.dep_ in ("nsubj", "nsubjpass")]
        obj = [w for w in token.rights if w.dep_ in ("dobj", "attr")]
        if subject and obj:
            print(f"'{subject[0]}' {token.lemma_} '{obj[0]}'")

# 2. Análise de Sentimentos com Transformers
print("\n[2] Análise de Sentimentos com Transformers:")
sentiment = pipeline("sentiment-analysis")
sentences = sent_tokenize(text)
for s in sentences:
    result = sentiment(s)[0]
    print(f"[{result['label']}] ({result['score']:.2f}) -> {s}")

# 3. Agrupamento e Extração de Tópicos com Embeddings
print("\n[3] Agrupamento e Tópicos com embeddings (BERT + KMeans):")
embedder = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedder.encode(sentences)

n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(embeddings)

# Mostrar agrupamento
for i, sentence in enumerate(sentences):
    print(f"[Cluster {clusters[i]}] {sentence}")

# Redução PCA para visualização
pca = PCA(n_components=2)
reduced = pca.fit_transform(embeddings)
plt.figure(figsize=(8, 5))
plt.scatter(reduced[:, 0], reduced[:, 1], c=clusters, cmap='rainbow')
plt.title("Agrupamento de sentenças via KMeans + BERT")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.grid(True)
plt.show()

# 4. Resumo Automático com Transformers
print("\n[4] Resumo Automático com Transformers:")
summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")
summary = summarizer(text, max_length=100, min_length=30, do_sample=False)
print(summary[0]['summary_text'])

np.random.seed(42)
np.random.randint(100, size=6)

"""RandInt ajuda a gerar números aleatórios intervalados e constantes"""