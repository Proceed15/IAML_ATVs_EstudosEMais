# -*- coding: utf-8 -*-
"""IAAM Aula 02 - PNL.ipynb

Automatically generated by Colab.

Original file is located at Google Colab
"""

!python -m spacy download pt_core_news_sm
import spacy
import nltk
nltk.download('rslp')

# Tokenização

nlp = spacy.load("pt_core_news_sm")
texto = "Olá! Bem-vindo ao curso de PNL. Vamos aprender sobre tokenização. Eu gosto de PNL!"
texth = "Hello There! HTML is a markup language! See you next week with more facts!"
doc = nlp(texto)
hhhhhhh = nlp(texth)

tokens = [token.text for token in doc]
LrTokens = [token.text for token in hhhhhhh]
print("Tokens:", tokens)
print("LrTokens:", LrTokens)

sentencas = [sent.text for sent in doc.sents]
sentences = [sent.text for sent in hhhhhhh.sents]
print("Sentenças:", sentencas)
print("Sentences:", sentences)

# Expansão de contração

import re

contracoes = {
    "tá": "está",
    "tô": "estou",
    "cê": "você",
    "num": "em um",
    "pra": "para a",
    "pro": "para o",
    "do": "de o",
    "da": "de a"
}

def expandir_contracoes(texto):
    palavras = texto.split()
    palavras_expandidas = [contracoes.get(p, p) for p in palavras]
    return " ".join(palavras_expandidas)

texto = "Cê tá num lugar bonito, mas num vou pra casa."
texto_expandido = expandir_contracoes(texto.lower())

print("Texto original:", texto)
print("Texto expandido:", texto_expandido)

# Normalização
import re

def normalizar_texto(texto):
    texto = texto.lower()  # Converte para minúsculas
    texto = re.sub(r'[^\w\s]', '', texto)  # Remove pontuações
    texto = re.sub(r'\d+', '', texto)  # Remove números
    return texto

texto = "O preço do produto é R$ 199,90!!! Isso é incrível."
texto_normalizado = normalizar_texto(texto)
print("Texto normalizado:", texto_normalizado)

# Stopwords
stopwords = nlp.Defaults.stop_words  # Lista de stopwords do SpaCy

for sentenca in doc.sents:
    tokens = [token.text for token in sentenca]  # Tokeniza a sentença
    tokens_filtrados = [token.text for token in sentenca if token.text.lower() not in stopwords]

for sentences in hhhhhhh.sents:
    tokens = [token.text for token in sentences] # Tokenize the Sentence
    filtered_tokens = [token.text for token in sentences if token.text.lower() not in stopwords]
    print(f"Sentença original: {sentenca.text}")
    print(f"Texto sem stopwords: {tokens_filtrados}")
    print("-" * 50)
    print(f"Original Sentence: {sentences.text}")
    print(f"Filtered Text: {filtered_tokens}")
    print("=" * 60)

# Stemming
from nltk.stem import RSLPStemmer

stemmer = RSLPStemmer()
palavras = ["correr", "correndo", "corrida", "corredor"]
stems = [stemmer.stem(palavra) for palavra in palavras]

print("Palavras da lista correr correndo corrida corredor após stemming:\n", "++++++++++++++>",stems)

# Exercício Resolvido
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

# Texto de entrada
#texto = "O estudante estava estudando programação e programando novos projetos."
texto = "C# is a Programming Language"

# Tokenização
tokens = word_tokenize(texto)

# Remoção de stopwords
#stop_words = set(stopwords.words('portuguese'))
stop_words = set(stopwords.words('english'))
tokens_sem_stopwords = [word for word in tokens if word.lower() not in stop_words]

# Stemming
stemmer = RSLPStemmer()
tokens_stem = [stemmer.stem(word) for word in tokens_sem_stopwords]

print("Tokens:", tokens)
print("Sem Stopwords:", tokens_sem_stopwords)
print("Após Stemming:", tokens_stem)

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer

#nltk.download('punkt')
#nltk.download('punkt_tab')
#nltk.download('stopwords')

# Texto de entrada
texto = "A Chama Estava Acesa e Acenderam Novamente"

# Tokenização
tokens = word_tokenize(texto)

# Remoção de stopwords
stop_words = set(stopwords.words('portuguese'))
tokens_sem_stopwords = [word for word in tokens if word.lower() not in stop_words]

# Stemming
stemmer = RSLPStemmer()
tokens_stem = [stemmer.stem(word) for word in tokens_sem_stopwords]

print("Tokens:", tokens)
print("Sem Stopwords:", tokens_sem_stopwords)
print("Após Stemming:", tokens_stem)